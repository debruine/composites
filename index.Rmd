---
title             : "The Composite Method Produces High False Positive Rates"
shorttitle        : "Composite Problems"

author: 
  - name          : "Lisa DeBruine"
    affiliation   : "1"
    corresponding : yes    # Define only one corresponding author
    address       : "62 Hillhead Street, Glasgow G12 8QB, Scotland"
    email         : "lisa.debruine@glasgow.ac.uk"
    role:         # Contributorship roles (e.g., CRediT, https://casrai.org/credit/)
      - Conceptualization
      - Funding acquisition
      - Methodology
      - Software
      - Validation
      - Visualization
      - Writing - Original Draft Preparation

affiliation:
  - id            : "1"
    institution   : "Institute of Neuroscience & Psychology, University of Glasgow"

authornote: |
  This research was funded by ERC grant #647910 (KINSHIP).

abstract: |
  Using a single pair or a small number of pairs of composite stimuli to assess the relationship between morphology and other traits is a common method in face research. Here, I use data simulation to demonstrate how this method inevitably leads to a high false positive rate, and how this problem is made worse by using a larger number of raters. I conclude by suggesting alternative methods for assessing the relationship between face morphology and individual traits.
  
keywords          : "faces; morphing; composite; average; mixed effects models"
wordcount         : "X"

bibliography      : ["r-references.bib","biblio.bib"]
csl               : apa.csl
floatsintext      : yes
figurelist        : no
tablelist         : no
footnotelist      : no
linenumbers       : yes
mask              : no
draft             : yes

documentclass     : "apa6"
classoption       : "man"
output            : 
  bookdown::html_document2:
    toc: true
    toc_float: true
    number_sections: false
    self_contained: false
    code_folding: hide
note              : "Draft"
---

```{r setup, include = FALSE}
# devtools::install_github("crsh/papaja")
library(papaja) # for manuscript formatting
library(kableExtra) # for table formatting
library(dplyr) # for data wrangling
library(tidyr) # for data wrangling
library(ggplot2) # for data visualisation
library(purrr) # for iteration
library(faux) # for data simulation
library(broom) # for tidy analyses
library(glue) # for reproducible text
library(pwr) # for power analyses
library(lme4) # for mixed effects models
library(lmerTest) # for p-values in LMEM

# devtools::install_github("debruine/webmorphR.stim")
library(webmorphR) # for reproducible stimuli
library(webmorphR.stim) # for additional stimuli

# update R package bibliography
#r_refs("r-references.bib", append = FALSE)

knitr::opts_chunk$set(
  cache       = FALSE,
  cache.extra = knitr::rand_seed,
  out.width   = "100%",
  warning     = FALSE,
  message     = FALSE,
  echo        = FALSE,
  fig.width   = 8,
  fig.height  = 4
)

wm_opts(plot.maxwidth = 850)
faux_options(plot = FALSE)
theme_set(theme_minimal())
```


Composite images (also called average or prototype images) can be created by morphing with software such as Psychomorph [@benson1991perception;@benson1991synthesising;@benson1993extracting;@tiddeman2001prototyping] or WebMorph [@webmorph;@R-webmorphR]. They can be a useful way to visualise the differences between groups of images (Figure \@ref(fig:demo-composites)).

```{r demo-composites, fig.cap="Example composite images, each comprising 4 faces of a specific gender and ethnic group."}
composites <- load_stim_composite(c(1:2, 4:7, 9:10))
plot_stim(composites, nrow = 2)
```

Many studies have used composite images to investigate the link between face morphology and various traits, such as cooperation in an economic game [@Little2013PID], sexual strategies [@Boothroyd2008EHB], voice pitch [@Feinberg2005AB], ability to elicit gaze-cueing [@Jones2011SJP], or dark triad personality traits [@alper2021all;@holtzman2011facing]. Typically, a pair of face composites is created from two groups, such as cooperators and defectors in a prisoners' dilemma game, or faces are rank-ordered on a continuous trait, such as score on a narcissism questionnaire, and some proportion of the top and bottom scorers are averaged together. Then, raters either rate the individual composites or assess which composite in the pair appears higher on some judgement of interest. This judgement can be related to the difference between the composites, such as cooperativeness or narcissism, or it can be another judgement that is hypothesised to be associated with the difference, such as attractiveness or dominance. If one image in the pair is rated significantly higher than the other image on the judgement in question, this is taken as evidence that the trait is associated with face morphology eliciting that judgement.

Despite its common use in face research, my own past research included, here I argue that this method produces extremely high false positive rates. Under not-unusual conditions, the false positive rate can near 50% for directional hypotheses and 100% for non-directional hypotheses. 

## Birthdate and Height

To explain why, I'll start with an analogy that has nothing to do with faces (bear with me). Imagine a researcher predicts that women born on odd days are taller than women born on even days. Ridiculous, right? So let's simulate some data assuming that isn't true (see <https://github.com/debruine/composites> for the code used to create the examples in this paper). We will sample 20 women from a population with a mean height of 162 cm and a standard deviation of 7 (values for women in Scotland). Half are born on odd days and half on even days.

```{r}
stim_n <- 10
height_m <- 162
height_sd <- 7

set.seed(42) # for reproducible simulation values
odd <- rnorm(stim_n, height_m, height_sd)
even <- rnorm(stim_n, height_m, height_sd)

t <- t.test(odd, even, alternative = "greater")
```


```{r}
e <- effectsize::cohens_d(odd, even)
stats <- glue::glue("$t_{{{apa_num(t$parameter)}}}$ = {apa_num(t$statistic)}, $p$ = {apa_p(t$p.value)}, $d$ = {apa_num(e$Cohens_d)}")
```

A t-test shows no significant difference (`r stats`), which is unsurprising. We simulated the data from the same distribution, so we know for sure there is no real difference here. 

Now we're going to average the height of the women with odd and even birthdays. So if we create a full-body composite of women born on odd days, she would be `r mean(odd) |> round(1)` cm tall, and a composite of women born on even days would be `r mean(even) |> round(1)` cm tall. If we ask raters to look at these two composites, side-by-side, and judge which one looks taller, what do you imagine would happen? It's likely that nearly all of them would judge the odd-birthday composite as taller. 

But let's say that raters have to judge the composites independently, and they are pretty bad with height estimation, so their estimates for each composite have error with a standard deviation of 10 cm. We can simulate such ratings from 50 raters and then compare the estimates for the odd-birthday composite with the estimates for the even-birthday composite.

```{r}
rater_n <- 50 # number of raters
error_sd <- 10 # rater error

# add the error to the composite mean heights
# don't bother simulating within-rater correlations to be conservative
set.seed(1) # for reproducible simulation values
odd_est  <- mean(odd)  + rnorm(rater_n, 0, error_sd)
even_est <- mean(even) + rnorm(rater_n, 0, error_sd)

t <- t.test(odd_est, even_est, paired = TRUE, alternative = "greater")
```

```{r}
e <- effectsize::cohens_d(odd_est, even_est)
stats <- glue::glue("$t_{{{t$parameter}}}$ = {apa_num(t$statistic)}, $p$ = {apa_p(t$p.value)}, $d$ = {apa_num(e$Cohens_d)}")
```

Now the women with odd birthdays are significantly taller than the women with even birthdays (`r stats`)! What changed? Essentially, we're no longer testing whether women born on odd days are taller than those born on even days, but whether raters can perceive the chance difference in height between the pair of composites. As long as there is any difference between the composites that exceeds the perceptual threshold for detection, we can find a significant result with enough raters. The effect has a 50% chance of being in the predicted direction, and whatever result we find with this face pair is likely to be highly replicable in a new set of raters rating the same face pair.

```{r}
set.seed(13)
simexp <- function(stim_n = 10, rater_n = 50) {
  error_sd <- 10 # rater error
  height_m <- 162
  height_sd <- 7
  
  odd <- rnorm(stim_n, height_m, height_sd)
  even <- rnorm(stim_n, height_m, height_sd)
  t <- t.test(odd, even, alternative = "greater")

  odd_est  <- mean(odd)  + rnorm(rater_n, 0, error_sd)
  even_est <- mean(even) + rnorm(rater_n, 0, error_sd)

  t_est <- t.test(odd_est, even_est, paired = TRUE, alternative = "greater")
  t_nd <- t.test(odd_est, even_est, paired = TRUE)
  
  list(Individual = t$p.value,
       CompositeD = t_est$p.value,
       CompositeND = t_nd$p.value)
}

simdat <- map_df(1:10000, ~simexp())

ind_false_positives <- (100 * mean(simdat$Individual < .05)) |> round(2)
comp_false_positives <- (100 * mean(simdat$CompositeD < .05)) |> round(1)
nd_false_positives <- (100 * mean(simdat$CompositeND < .05)) |> round(1)
```


Maybe this is just a fluke of the original sample? We can repeat the procedure above 10000 times and check the p-values of the individual analysis versus the composite method. We can see that the individual method has the expected uniform distribution of p-values (Figure \@ref(fig:ind-comp)), as there is no difference between the two groups. The proportion of false positives is `r ind_false_positives`%, which is close to the alpha criterion of 0.05. However, the composite method produced a false positive rate of `r comp_false_positives`% with a directional hypothesis, and `r nd_false_positives`% with a non-directional hypothesis. And as we'll see later, you can increase the false positive rate to near 50% for directional hypotheses and 100% for non-directional hypotheses by increasing the number of raters.


```{r ind-comp, fig.cap="Individual versus composite method. The individual method shows the expected uniform distribution of p-values, while the composite method has an inflated false positive rate."}
simdat |>
  mutate(id = row_number()) |>
  pivot_longer(Individual:CompositeND) |>
  ggplot(aes(x = value, color = name)) +
  geom_freqpoly(aes(y = (..count..)/10000), size = 1.5,
                binwidth = 0.05, boundary = 0) +
  scale_x_continuous(breaks = seq(0, 1, .1), 
                     minor_breaks = seq(0, 1, .05),
                     limits = c(0, 1)) +
  scale_y_continuous(labels = scales::percent) +
  scale_color_manual(values = c("goldenrod2", "hotpink", "skyblue"),
                     labels = c("Composite (directional)",
                                "Composite (non-directional)",
                                "Individual")) +
  labs(x = "p-value", y = "Percent of p-values in each 5% bin", 
       color = "Method") +
  theme(legend.position = c(.5, .7))
```



## Simulating a Real Example

A recent paper by @alper2021all used faces from the Faceaurus database [@holtzman2011facing] to test whether dark triad personality traits (Machiavellianism, narcissism, and psychopathy) are visible in the face. “Holtzman (2011) standardized the assessment scores, computed average scores of self- and peer-reports, and ranked the face images based on the resulting scores. Then, prototypes for each of the personality dimensions were created by digitally combining 10 faces with the highest, and 10 faces with the lowest scores on the personality trait in question (Holtzman, 2011).” This was done separately for male and female faces.

With 105 raters, Holtzman found that the ability to detect the composite higher in a dark triad trait was greater than chance for all three traits for both genders investigated. Alper and colleagues replicated these findings in three studies with rater numbers of 160, 318, and 402, the larger two of which were pre-registered.

While I commend both @holtzman2011facing and @alper2021all for their transparency, data sharing, and material sharing, I argue that the original test has an effective N of 2, not 105, and that further replications using these images, regardless of number of raters or preregistered status, lend no further weight of evidence to the assertion that dark triad traits are visible in physical appearance.

### Simulating Rating Data

To explain why, let's simulate 100 datasets of self- and peer-assesed dark triad scores with the same  structure as the original study. Each simulated dataset will have 48 women and 33 men whose Machiavellian, narcissism, NPD, and psychopathy scores are correlated in the same way as @holtzman2011facing. 

```{r}
# correlations from Holtzman 2011

# convert lower left triangle to upper right
m <- matrix(NA, 8, 8)
m[upper.tri(m, diag = T)] <- c(
  1.00,
  0.34, 1.00,
  0.06, 0.56, 1.00,
  0.57, 0.40, 0.50, 1.00,
  0.16, 0.14, 0.41, 0.32, 1.00,
  0.17, 0.33, 0.56, 0.50, 0.47, 1.00,
  0.09, 0.33, 0.61, 0.52, 0.48, 0.77, 1.00,
  0.03, 0.11, 0.26, 0.41, 0.38, 0.43, 0.60, 1.00)
r_female <- t(m)[lower.tri(m)]

m <- matrix(NA, 8, 8)
m[upper.tri(m, diag = T)] <- c(
  1.00,
  0.05, 1.00,
  0.32, 0.64, 1.00,
  0.62, 0.38, 0.53, 1.00,
  0.22, 0.05, 0.01, 0.03, 1.00,
  0.06, 0.19, 0.17, 0.12, 0.37, 1.00,
  0.06, 0.22, 0.08, 0.22, 0.12, 0.75, 1.00,
  0.12, 0.19, 0.10, 0.13, 0.43, 0.49, 0.36, 1.00)
r_male <- t(m)[lower.tri(m)]
```


```{r}
set.seed(8675309) # for reproducible simulation values
nsim <- 100 # number of simulations

# simulate data
dat <- sim_design(
  n = c(48, 33),
  between = list(gender = c("female", "male")),
  within = list(
    type = c("self", "peer"),
    trait = c("mach", "narc", "npd", "psycho")
  ),
  r = list(female = r_female, male = r_male),
  dv = "score",
  empirical = TRUE,
  long = TRUE,
  rep = nsim
)
```


```{r, fig.cap="Correlation structure of the original data and simulations."}
get_params(dat$data[[1]], 
           between = "gender",
           within = c("type", "trait"), 
           dv = "score", id = "id") |>
  select(-mean, -sd, -n) |>
  pivot_longer(self_mach:peer_psycho) %>%
  mutate(name = factor(name, rev(levels(.$var)))) |>
  ggplot(aes(x = var, y = name, fill = value)) +
  geom_tile() +
  geom_text(aes(label = round(value, 2)), size = 3) +
  facet_wrap(~gender) +
  scale_fill_gradient(low = "white", high = "dodgerblue4", 
                      limits = c(0, 1)) +
  labs(x = NULL, y = NULL, fill = "Correlation") +
  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1))
```


### Simulate No Relationship

Next, calculate the average dark triad score for each subject and create a "dark triad face morphology" score to represent the extent to which each subject's face is perceived as high in dark triad traits. Importantly, in this simulation, the face morphology score will have *zero* correlation to the average dark triad score. Individual samples will, of course, show non-zero correlations between facial morphology and dark triad traits by chance alone, but these will tend to be small and non-directional.

```{r}
score_r <- 0 # correlation to the dark triad score

df <- dat |>
  mutate(data = map(data, \(x){
    face_signal <- x |>
      group_by(id) |>
      summarise(dark_triad = mean(score),
                .groups = "drop") |>
      mutate(face = rnorm_pre(dark_triad, r = score_r)) |>
      select(id, face)
    
    # average across self- and peer-reports
    x |>
      group_by(id, gender, trait) |>
      summarise(score = mean(score),
                .groups = "drop") |>
      left_join(face_signal, by = "id")
  }))
```


```{r, fig.cap="The first 8 simulated replicates, showing no systematic relationship between dark triad trait scores and facial morphology."}
df |>
  head(8) |>
  unnest(cols = data) |>
  ggplot(aes(x = score, y = face, color = gender)) +
  geom_point(alpha = 0.25, size = 0.5) +
  geom_smooth(method = lm, formula = y ~ x) +
  scale_color_manual(values = c("firebrick", "dodgerblue4")) +
  labs(x = "Average Dark Triad Trait Score",
       y = "Dark Triad Face Morphology") +
  facet_wrap(~rep, nrow = 2) +
  theme(strip.text = element_blank())
```



### Create Composites

Now pick the 10 images with the highest and lowest scores for each trait for each gender and create composites of these images. However, since scores on the three dark triad traits are positively correlated, the three pairs of composite faces are not independent. Indeed, @holtzman2011facing states that five individuals were in all three low composites for the male faces, while the overlap was less extreme in other cases.

```{r}
df$ten <- map(df$data, \(dat_avg) {
  # Pick the top and bottom 10 for each score
  top_ten <- dat_avg |>
    group_by(gender, trait) |>
    slice_max(order_by = score, n = 10) |>
    ungroup() |>
    mutate(rank = "top")
  
  bot_ten <- dat_avg |>
    group_by(gender, trait) |>
    slice_min(order_by = score, n = 10) |>
    ungroup() |>
    mutate(rank = "bottom")
  
  bind_rows(top_ten, bot_ten)
})

# average together the score and face morphology value for these groups of 10 faces.
df$avg <- map(df$ten, \(ten) {
  ten |>
    group_by(gender, trait, rank) |>
    summarise(score = mean(score),
              face = mean(face),
              .groups = "drop")
})
```

Here, we will assume that the face morphology that leads to perceptions of dark triad traits can be linearly combined. Even though this face morphology is totally unrelated to the dark triad personality scores, the composites will still differ in this morphology, some more than others, and half the time in the predicted direction.

```{r reps-diff, fig.cap="Differences in average dark triad face morphology between the high and low dark triad trait groups for the first 8 replicates."}

triad_colors <- c("darkorange", "darkgreen", "orchid4")
triad_names <- c("Machiaveliansim", "Narcissism", "Psychopathy")

df |>
  head(8) |>
  select(rep, avg) |>
  unnest(avg) |>
  filter(trait != "npd") |>
  ggplot(aes(x = rank, y = face, color = trait, shape = gender)) +
  geom_line(aes(group = paste(trait, gender))) +
  geom_point(size = 2) +
  facet_wrap(~rep, nrow = 2) +
  scale_color_manual(values = triad_colors, labels = triad_names) +
  labs(y = "Average Dark Triad Face Morphology", 
       color = "Trait", shape = "Gender") +
  theme(strip.text = element_blank())
```

```{r}
# Calculate the difference between the face morphology average scores for the top and bottom in each gender/trait combo.

df$diff <- map(df$avg, \(avg) {
  avg  |>
    filter(trait != "npd") |>
    select(-score) |>
    pivot_wider(names_from = rank, values_from = face) |>
    mutate(diff = top - bottom)
})
```

```{r diff-dist, fig.cap="The distribution of the difference between high and low composites in average dark triad face morphology, across the 100 replicates. The blue line shows the minumum effect size for which there is 80% power for 105 raters to detect the difference."}
d_sensitivity <- pwr::pwr.t.test(n = 105, power = 0.8, 
                                 type = "one.sample",
                                 alternative = "greater")$d

df |>
  select(rep, diff) |>
  unnest(diff) |>
  ggplot(aes(x = gender, y = diff, fill = trait)) +
  geom_hline(yintercept = d_sensitivity, color = "dodgerblue4") +
  geom_violin(alpha = 0.5) +
  scale_fill_manual(values = triad_colors, labels = triad_names) +
  labs(x = NULL, y = "Difference between High and Low Composites", fill = "Trait")
```



### Simulate Composite Ratings

Following Holtzman, we will simulate raters for each replicate giving -5 to +5 ratings for which face in each pair looks more Machiavellian, narcissistic, or psychopathic. Each pairing will be rated twice by each rater.

```{r}
set.seed(0) # for reproducible simulation values
df$raters <- map(df$diff, \(diffs) {
  sim_design(
    n = 105,
    within = list(rep = 1:2,
                  gender = c("female", "male"),
                  trait = c("mach", "narc", "psycho")),
    long = TRUE
  ) |>
    # convert to a truncnorm distribution from -5.49 to 5.49 
    # so rounding produces values -5:5
    # assume an SD of 1.5 (approximated from values in Holtzman, 2011)
    mutate(diff = case_when(
      gender == "female" & trait == "mach"   ~ diffs$diff[[1]],
      gender == "female" & trait == "narc"   ~ diffs$diff[[2]],
      gender == "female" & trait == "psycho" ~ diffs$diff[[3]],
      gender == "male"   & trait == "mach"   ~ diffs$diff[[4]],
      gender == "male"   & trait == "narc"   ~ diffs$diff[[5]],
      gender == "male"   & trait == "psycho" ~ diffs$diff[[6]]
    )) |>
    group_by(gender, trait) |>
    mutate(rating = norm2trunc(y, -5.49, 5.49, diff, 1.5) |> round()) |>
    ungroup() |>
    group_by(id, gender, trait) |>
    summarise(rating = mean(rating),
              .groups = "drop")
})
```

By chance alone, some of the values will be significant in the predicted direction.

```{r}
df$stats <- map(df$raters, \(raters) {
  raters |>
    group_by(gender, trait) |>
    nest() |>
    mutate(analysis = map(data, \(x) {
      t.test(x$rating, type = "one.sample",
             alternative = "greater") |> 
        broom::tidy()
    })) |>
    select(-data) |>
    unnest(analysis) 
})
  
```



```{r, fig.cap="The distribution of ratings for the first 8 replicates. Thicker lines show the gender-trait conditions that showed significant differences in a one-sample t-test.", eval = FALSE}
nsample <- 8
stats <- df |>
  head(nsample) |>
  select(rep, stats) |>
  unnest(stats) |>
  mutate(sig = ifelse(p.value < .05, 1, 0.25))
  
df |>
  head(nsample) |>
  select(rep, raters) |>
  unnest(raters) |>
  left_join(stats, by = c("rep", "gender", "trait")) |>
  ggplot(aes(gender, rating, fill = trait)) +
  geom_violin(aes(linewidth = sig), adjust = 1.5, alpha = 0.5) +
  stat_summary(size = .25, fun.data = mean_cl_normal,
               position = position_dodge(width = 0.9),
               show.legend = FALSE) +
  geom_hline(yintercept = 0, color = "grey", linewidth = 0.25) +
  scale_fill_manual(values = triad_colors, labels = triad_names) +
  scale_linewidth_identity() +
  facet_wrap(~rep, nrow = 2) +
  labs(x = NULL, y = "Ratings", fill = "Trait") +
  theme(strip.text = element_blank())
```

```{r, fig.cap="Distribution of replicates with 0 to 6 significant results in the predicted direction (one-tailed one-sample t-tests with alpha = 0.05)."}
all_stats <- df |>
  select(rep, stats) |>
  unnest() |>
  select(rep:p.value) |>
  mutate(sig = p.value < .05) |>
  mutate(across(where(is.numeric), round, 3))

all_stats |>
  group_by(rep) |>
  summarise(sig = sum(sig)) |>
  ggplot(aes(x = sig)) +
  geom_histogram(binwidth = 1, color = "black", fill = "dodgerblue") +
  scale_x_continuous("Number of Significant Tests",
                     breaks = 0:6) +
  coord_cartesian(xlim = c(-0.5, 6.5)) +
  scale_y_continuous("Number of Simulations")
```

People tend to show high agreement on stereotypical social perceptions from the physical appearance of faces, even when physical appearance is not meaningfully associated with the traits being judged [@todorov2008understanding;@zebrowitz2008social;@jones2021world]. We can be sure that by chance alone, our two composites will be at least slightly different on any measure, even if they are drawn from identical populations. 

## More Raters is Even Worse

A naive solution to this problem is to increase the number of raters, which should produce more accurate results, right? Actually, this makes the problem even worse. As you increase the number of raters, the power to detect even small (chance) differences in composites rises (Figure \@ref(fig:power-curves)). Consequently, you can virtually guarantee significant results, even for tiny differences or traits that people are very bad at estimating. 

```{r power-curves, fig.cap="Power curves for a one-tailed, one-sample t-test.", fig.width = 7, fig.height = 5}

x <- crossing(
  n = seq(5, 500, 5),
  d = seq(.1, .5, .1)
) |>
  mutate(power = pwr::pwr.t.test(n, d, type = "one.sample",
                                 alternative = "greater")$power)

ggplot(x, aes(n, power, color = factor(d))) +
  #geom_point() +
  geom_line(size = 0.75) +
  scale_x_continuous(breaks = seq(0, 500, 100)) +
  scale_y_continuous(breaks = seq(0, 1, .1)) +
  scale_color_viridis_d() +
  labs(x = "Number of Raters",
       y = "Power",
       color = "Effect Size") +
  coord_cartesian(ylim = c(0, 1), xlim = c(0, 500)) +
  theme(legend.position = c(.85, .2))
```

```{r}
set.seed(0) # for reproducible simulation values
chance_diff <- data.frame(n = seq(5, 50, 5)) |>
  mutate(diff = map(n, \(n) {
    replicate(10000, mean(rnorm(n))-mean(rnorm(n)))
  })) |>
  unnest(diff) |>
  mutate(unsigned = abs(diff))

median_unsigned_diff_10 <- chance_diff |> 
  filter(n == 10) |>
  pull(unsigned) |>
  median() |>
  round(2)
```

How likely is it that there will be chance differences in the composites big enough to be a problem? More likely than you probably think, especially when there are a small number of stimuli in each composite. The smaller the number of stimuli that go into each composite, the larger the median (unsigned) size of this difference (Figure \@ref(fig:composite-size)). With only 10 stimuli per composite (like the Facesaurus composites), the median unsigned effect size of the difference between composites from populations with no real difference is `r median_unsigned_diff_10` (in units of SD of the original trait distribution). If our raters are accurate enough at perceiving this difference, or we run a very large number of raters, we are virtually guaranteed to find significant results every time. There is a 50% chance that these results will be in the predicted direction, and this direction will be replicable across different samples of raters for the same image set.

```{r composite-size, fig.cap="Simulated data showing the distribution of effect sizes for the difference between pairs of composites sampled from the same distribution (i.e., no real effect). Points show the median unsigned effect size."}
ggplot(chance_diff, aes(x = n, y = unsigned)) +
  geom_hline(yintercept = 0, color = "grey70") +
  geom_violin(aes(group = factor(n), y = diff, fill = n), alpha = 0.5) +
  stat_summary(geom = "point", fun.y = median) +
  scale_fill_viridis_c() +
  scale_x_continuous(breaks = seq(0, 50, 5)) +
  labs(x = "Number of stimuli per composite",
       y = "Effect size of difference between pairs") +
  theme(legend.position = "none")
```

## Implications for Face Research

So what does this mean for studies of the link between personality traits and facial appearance? The analogy with birth date and height holds. As long as there are facial morphologies that are even slightly consistently associated with the *perception* of a trait, then composites will not be identical in that morphology. Thus, even if that morphology is totally unassociated with the trait as measured by, e.g., personality scales or peer report (which is often the case), using the composite rating method will inflate the false positive rate for concluding a difference.  

The smaller the number of stimuli that go into each composite, the greater the chance that they will be visibly different in morphology related to the judgement of interest, just by chance alone. The larger the number of raters or the better raters are at detecting small differences in this morphology, the more likely that "detection" will be significantly above chance. Repeating this with a new set of raters does not increase the amount of evidence you have for the association between the face morphology and the measured trait. You've only measured it once in one population of faces. If raters are your unit of analyses, you are making conclusions about whether the population of raters can detect the difference between your stimuli, you cannot generalise this to new stimulus sets.

So how should researchers test for differences in facial appearance between groups? Here I discuss two alternative methods for investigating the relationship between traits and face morphology.

### Assessment of individual faces

Assessment of individual face images, combined with analysis using mixed effects models, can allow you to simultaneously account for variance in both raters and stimuli, avoiding the inflated false positives of the composite method and the similar problem that occurs when ratings of individual stimuli are averaged before analysis [@barrgeneralizing]. People often use the composite method when they have too many images for any one rater to rate, but cross-classified mixed models can analyse data from counterbalanced trials or randomised subset allocation. 

Here we simulate data from a design where 200 faces from two trait groups are rated by 200 raters, in 10 counterbalanced batches, such that each rater only rates 10 faces from each trait group. 

```{r}
n_faces <- 200 # number of faces
n_raters <- 200 # number of raters
cb_groups <- LETTERS[1:10] # counterbalanced groups
b1 <- 0.5 # fixed effect size for trait
face_b0_sd <- 1 # SD of random intercept for faces
rater_b0_sd <- 1 # SD of random intercept for raters
rater_b1_sd <- 1 # SD of random slope of trait effect for raters
rater_cors <- 0.5 # correlation for rater random effects
sigma <- 2 # SD of residual error

set.seed(4)
mixed_df <- add_random(face = n_faces) |>
  add_random(rater = n_raters) |>
  
  # each rater rates faces from 1 of 10 counterbalanced groups
  add_between("face", trait = c("low", "high"),
              cb_face = cb_groups) |>
  add_between("rater", cb_rater = cb_groups) |>
  filter(cb_face == cb_rater) |>
  
  # set random effects
  add_ranef("face", face_b0 = face_b0_sd) |>
  add_ranef("rater", 
            rater_b0 = rater_b0_sd, 
            rater_b1 = rater_b1_sd, 
            .cors = rater_cors) |>
  add_ranef(err = sigma) |>
  
  # contrast code factors and create DV
  add_contrast("trait", contrast = "treatment", colnames = "trait.c") |>
  mutate(rating = face_b0 + rater_b0 + (b1 + rater_b1) * trait.c + err) |>
  select(face:cb_face, trait.c, rating)
```

The following mixed effects analysis accounts for the structure of the data above. Each rater does not have to rate each face in order for random effects of face and rater to be accounted for. See @debruine2021understanding for further discussion of the benefits of mixed effects models for this type of experimental design.

```{r}
m <- lmer(rating ~ trait + 
            (1 | face) + 
            (1 + trait | rater), 
          data = mixed_df)

summary(m)
```


### Random Face Pairs

Another reason to use the composite rating method is when you are not ethically permitted to use individual faces in research, but are ethically permitted to use non-identifiable composite images. In this case, you can generate a large number of random composite pairs to construct the chance distribution. The equivalent to a p-value for this method is the proportion of the randomly paired composites that your target pair has a less extreme result than. While this method is too tedious to use when constructing composite faces manually, scripting with webmorphR [@R-webmorphR] allows you to automate such a task.

```{r, eval = FALSE}
set.seed(8) # for reproducibility

# load 20 faces
f <- load_stim_canada("f") |> resize(0.5)

# set to the number of random pairs you want
n_pairs <- 5

# repeat this code n_pairs times
pairs <- lapply(1:n_pairs, function (i) {
  # sample a random 10:10 split
  rand1 <- sample(names(f), 10)
  rand2 <- setdiff(names(f), rand1)
  
  # create composite images
  comp1 <- avg(f[rand1])
  comp2 <- avg(f[rand2])
  
  # save images with paired names
  nm1 <- paste0("img_", i, "_a")
  nm2 <- paste0("img_", i, "_b")
  write_stim(comp1, dir = "images/composites", names = nm1)
  write_stim(comp2, dir = "images/composites", names = nm2)
})
```

```{r rand-pair, fig.cap = "Five random pairs of composites from a sample of 20 faces (10 in each composite). Can you spot any differences?"}
pairs <- read_stim("images/composites/")
plot(pairs, byrow = FALSE, nrow = 2)
```



## Open Resources

Face images are from the open-source, CC-BY licensed image set, the Face Research Lab London Set [@FRL_London]. All software is available open source. The code to reproduce this paper can be found at https://github.com/debruine/composites. 

We used `r cite_r("r-references.bib")` to produce this manuscript.


\newpage

## References

\begingroup
\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}

<div id="refs" custom-style="Bibliography"></div>
\endgroup
